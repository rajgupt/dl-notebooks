{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras-text-classification.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyM0KDMZZTLi+B4PBSksAEGd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajgupt/dl-notebooks/blob/main/keras_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqoamizG7yMy"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkWsWI1j8sXn"
      },
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPhBF3Px8syn"
      },
      "source": [
        "!ls aclImdb/train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fr6bFGBl-DZO"
      },
      "source": [
        "!rm -r aclImdb/train/unsup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6u_mFrgu81FW"
      },
      "source": [
        "bs = 32\n",
        "train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    \"aclImdb/train\",\n",
        "    batch_size=bs,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",seed=42\n",
        ")\n",
        "train_ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7Bagv0y9wfv"
      },
      "source": [
        "val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    \"aclImdb/train\",\n",
        "    batch_size=bs,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=42,\n",
        ")\n",
        "test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=bs\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AT4TyJrN-bNQ"
      },
      "source": [
        "tf.data.experimental.cardinality(train_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mo9Jhl_0-jkv"
      },
      "source": [
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "import string\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E13Pj6Uw-uBe"
      },
      "source": [
        "def custom_standardization(input_data):\n",
        "    lowercase = tf.strings.lower(input_data)\n",
        "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
        "    return tf.strings.regex_replace(\n",
        "        stripped_html, \"[%s]\" % re.escape(string.punctuation), \"\"\n",
        "    )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qlKc6gd_OrL"
      },
      "source": [
        "# Model constants.\n",
        "max_features = 20000\n",
        "embedding_dim = 128\n",
        "sequence_length = 500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iPVTz2-_WEo"
      },
      "source": [
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=max_features,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TB60ttnm_eZC"
      },
      "source": [
        "# Let's make a text-only dataset (no labels):\n",
        "text_ds = train_ds.map(lambda x, y: x)\n",
        "# Let's call `adapt`:\n",
        "vectorize_layer.adapt(text_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5mdqVvv_kfs"
      },
      "source": [
        "def vectorize_text(text, label):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    return vectorize_layer(text), label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGWlEkYLDDdr"
      },
      "source": [
        "# Vectorize the data.\n",
        "train_ds = train_ds.map(vectorize_text)\n",
        "val_ds = val_ds.map(vectorize_text)\n",
        "test_ds = test_ds.map(vectorize_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLPdK7bO_7ab"
      },
      "source": [
        "# Do async prefetching / buffering of the data for best performance on GPU.\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=10)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=10)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsCR7ssmENuq"
      },
      "source": [
        "# Build model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUSz3XD3D0Uu"
      },
      "source": [
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0T6aomaD14b"
      },
      "source": [
        "input = tf.keras.Input(shape=(None,), dtype='int64')\n",
        "x = layers.Embedding(max_features, embedding_dim)(input)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "# Conv1D\n",
        "x = layers.Conv1D(128,7,padding='valid',activation='relu',strides=3)(x)\n",
        "x = layers.Conv1D(128,7,padding='valid',activation='relu',strides=3)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "\n",
        "x = layers.Dense(128, activation='relu')(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "output = layers.Dense(1,activation='sigmoid')(x)\n",
        "\n",
        "model = tf.keras.Model(input, output)\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HytJB3BEeWL"
      },
      "source": [
        "model.fit(train_ds, validation_data=val_ds, epochs=3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPNTG7OUFwyR"
      },
      "source": [
        "model.evaluate(test_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fw6F9coaF4mr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}